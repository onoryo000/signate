{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\", index_col=0) # 学習用データ\n",
    "test = pd.read_csv(\"test.csv\", index_col=0)   # 評価用データ\n",
    "sample_submit = pd.read_csv(\"sample_submit.csv\", index_col=0, header=None) # 応募用サンプルファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corrwith(train[\"Attrition\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "BusinessTravel={\"Non-Travel\":1,\"Travel_Frequently\":2, \"Travel_Rarely\":3}\n",
    "train[\"BusinessTravel\"]=train[\"BusinessTravel\"].map(BusinessTravel)\n",
    "test[\"BusinessTravel\"]=test[\"BusinessTravel\"].map(BusinessTravel)\n",
    "\n",
    "Department={\"Human Resources\":1,\"Research & Development\":2,\"Sales\":3}\n",
    "train[\"Department\"]=train[\"Department\"].map(Department)\n",
    "test[\"Department\"]=test[\"Department\"].map(Department)\n",
    "\n",
    "EducationField={\"Human Resources\":1,\"Life Sciences\":2,\"Marketing\":3,\"Medical\":4,\"Other\":5,\"Technical Degree\":6}\n",
    "train[\"EducationField\"]=train[\"EducationField\"].map(EducationField)\n",
    "test[\"EducationField\"]=test[\"EducationField\"].map(EducationField)\n",
    "\n",
    "\n",
    "Gender={\"Female\":1,\"Male\":2}\n",
    "train[\"Gender\"]=train[\"Gender\"].map(Gender)\n",
    "test[\"Gender\"]=test[\"Gender\"].map(Gender)\n",
    "\n",
    "\n",
    "MaritalStatus={\"Divorced\":1,\"Married\":2,\"Single\":3}\n",
    "train[\"MaritalStatus\"]=train[\"MaritalStatus\"].map(MaritalStatus)\n",
    "test[\"MaritalStatus\"]=test[\"MaritalStatus\"].map(MaritalStatus)\n",
    "\n",
    "Over18={\"N\":1,\"Y\":2}\n",
    "train[\"Over18\"]=train[\"Over18\"].map(Over18)\n",
    "test[\"Over18\"]=test[\"Over18\"].map(Over18)\n",
    "\n",
    "OverTime={\"No\":1,\"Yes\":2}\n",
    "train[\"OverTime\"]=train[\"OverTime\"].map(OverTime)\n",
    "test[\"OverTime\"]=test[\"OverTime\"].map(OverTime)\n",
    "\n",
    "JobRole={\"Research Scientist\":1,\"Sales Executive\":2,\"Laboratory Technician\":3,\"Manufacturing Director\":4,\"Sales Representative\":5,\"Healthcare Representative\":6,\"Research Director\":7,\"Manager\":8,\"Human Resources\":9}\n",
    "train[\"JobRole\"]=train[\"JobRole\"].map(JobRole)\n",
    "test[\"JobRole\"]=test[\"JobRole\"].map(JobRole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corrwith(train[\"Attrition\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 標準化の対象となる特徴量を選択\n",
    "selected_features = [\"Age\", \"DailyRate\",\"DistanceFromHome\",\"EmployeeNumber\",\"HourlyRate\",\"MonthlyIncome\",\"NumCompaniesWorked\",\"PercentSalaryHike\",\"PerformanceRating\",\"RelationshipSatisfaction\",\"StandardHours\",\"StockOptionLevel\",\"TotalWorkingYears\",\"TrainingTimesLastYear\",\"WorkLifeBalance\",\"YearsAtCompany\",\"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\"]\n",
    "\n",
    "# StandardScalerのインスタンスを作成\n",
    "scaler = StandardScaler()\n",
    "# 選択した特徴量の平均値と標準偏差を計算し、標準化を実行\n",
    "train[selected_features] = scaler.fit_transform(train[selected_features])\n",
    "test[selected_features] = scaler.transform(test[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Attrition\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m      4\u001b[0m train\u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(train)\n\u001b[1;32m----> 5\u001b[0m test\u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(test)\n",
      "File \u001b[1;32mc:\\Users\\ryooo\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ryooo\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:508\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \n\u001b[0;32m    496\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 508\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    509\u001b[0m     X,\n\u001b[0;32m    510\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    511\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    512\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    513\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    514\u001b[0m )\n\u001b[0;32m    516\u001b[0m X \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[0;32m    517\u001b[0m X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\ryooo\\anaconda3\\lib\\site-packages\\sklearn\\base.py:529\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    465\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    466\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    471\u001b[0m ):\n\u001b[0;32m    472\u001b[0m     \u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ryooo\\anaconda3\\lib\\site-packages\\sklearn\\base.py:462\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    458\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    459\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m     )\n\u001b[1;32m--> 462\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Attrition\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train= scaler.fit_transform(train)\n",
    "test= scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train[\u001b[39m\"\u001b[39;49m\u001b[39mJobRole\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "train[\"JobRole\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train\u001b[39m.\u001b[39;49mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "test= test.drop([\"JobRole\"], axis=1)\n",
    "y = train[\"Attrition\"] # 目的変数\n",
    "X = train.drop([\"Attrition\",\"JobRole\"], axis=1) # 目的変数を除いたデータ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# LightGBMデータセットの作成\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# パラメータの設定\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.171875 -> initscore=-1.572397\n",
      "[LightGBM] [Info] Start training from score -1.572397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best num_boost_round: 48\n",
      "Best accuracy: 0.8541666666666666\n"
     ]
    }
   ],
   "source": [
    "best_ac = 0.0\n",
    "best_num_boost_round = 0\n",
    "\n",
    "for num_boost_round in range(1,50):\n",
    "    # モデルの学習\n",
    "    model = lgb.train(params, train_data, num_boost_round)\n",
    "    # テストデータの予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "    # 精度の評価\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    if accuracy > best_ac:\n",
    "        best_ac = accuracy\n",
    "        best_num_boost_round = num_boost_round\n",
    "        \n",
    "# 最適なnum_boost_roundを表示\n",
    "print(\"Best num_boost_round:\", best_num_boost_round)\n",
    "print(\"Best accuracy:\", best_ac)\n",
    "\n",
    "\n",
    "# model = lgb.train(params, train_data)\n",
    "# # model = LogisticRegression()\n",
    "# # model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_binary = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "\n",
    "# # 精度の評価\n",
    "# accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # 評価データの予測\n",
    "# pred = model.predict(test)\n",
    "# pred_binary = [1 if i >= 0.5 else 0 for i in pred]\n",
    "# pred_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit[1] = pred_binary\n",
    "sample_submit.to_csv('submit.csv', header=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
